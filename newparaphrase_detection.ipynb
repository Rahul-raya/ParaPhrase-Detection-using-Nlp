{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Rahul\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Rahul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SBERT model (better version)\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NLP tools\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    words = word_tokenize(text)  # Tokenize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & Stopword removal\n",
    "    return ' '.join(words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_csv('labeled_final_train.csv')  # Adjust filename if needed\n",
    "test = pd.read_csv('labeled_final_test.csv')\n",
    "\n",
    "# Preprocess text\n",
    "train['sentence1'] = train['sentence1'].apply(preprocess_text)\n",
    "train['sentence2'] = train['sentence2'].apply(preprocess_text)\n",
    "test['sentence1'] = test['sentence1'].apply(preprocess_text)\n",
    "test['sentence2'] = test['sentence2'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Encode text into SBERT embeddings\n",
    "train['sentence1_embedding'] = train['sentence1'].apply(lambda x: model.encode(x, convert_to_numpy=True))\n",
    "train['sentence2_embedding'] = train['sentence2'].apply(lambda x: model.encode(x, convert_to_numpy=True))\n",
    "test['sentence1_embedding'] = test['sentence1'].apply(lambda x: model.encode(x, convert_to_numpy=True))\n",
    "test['sentence2_embedding'] = test['sentence2'].apply(lambda x: model.encode(x, convert_to_numpy=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert embeddings to NumPy arrays\n",
    "train_embeddings1 = np.vstack(train['sentence1_embedding'].values)\n",
    "train_embeddings2 = np.vstack(train['sentence2_embedding'].values)\n",
    "test_embeddings1 = np.vstack(test['sentence1_embedding'].values)\n",
    "test_embeddings2 = np.vstack(test['sentence2_embedding'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    return 1 - cosine(vec1, vec2)\n",
    "\n",
    "# Compute extra features\n",
    "def sentence_length_diff(sent1, sent2):\n",
    "    return abs(len(sent1) - len(sent2))\n",
    "\n",
    "def jaccard_similarity(sent1, sent2):\n",
    "    words1, words2 = set(sent1.split()), set(sent2.split())\n",
    "    return len(words1 & words2) / len(words1 | words2) if words1 | words2 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute features\n",
    "train['similarity'] = [cosine_similarity(train_embeddings1[i], train_embeddings2[i]) for i in range(len(train))]\n",
    "train['length_diff'] = train.apply(lambda row: sentence_length_diff(row['sentence1'], row['sentence2']), axis=1)\n",
    "train['jaccard'] = train.apply(lambda row: jaccard_similarity(row['sentence1'], row['sentence2']), axis=1)\n",
    "\n",
    "test['similarity'] = [cosine_similarity(test_embeddings1[i], test_embeddings2[i]) for i in range(len(test))]\n",
    "test['length_diff'] = test.apply(lambda row: sentence_length_diff(row['sentence1'], row['sentence2']), axis=1)\n",
    "test['jaccard'] = test.apply(lambda row: jaccard_similarity(row['sentence1'], row['sentence2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot similarity distribution\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(train['similarity'], bins=30, kde=True, color='blue')\n",
    "plt.title('Cosine Similarity Distribution')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Density')\n",
    "plt.show()\n",
    "\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X_train = train[['similarity', 'length_diff', 'jaccard']].values\n",
    "y_train = train['label'].values\n",
    "X_test = test[['similarity', 'length_diff', 'jaccard']].values\n",
    "y_test = test['label'].values\n",
    "\n",
    "# Train Random Forest classifier\n",
    "classifier = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance plot\n",
    "feature_names = ['Cosine Similarity', 'Length Difference', 'Jaccard Similarity']\n",
    "feature_importances = classifier.feature_importances_\n",
    "plt.figure(figsize=(7,5))\n",
    "sns.barplot(x=feature_importances, y=feature_names, palette=\"Blues_r\")\n",
    "plt.title(\"Feature Importance\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Not Paraphrase', 'Paraphrase'], yticklabels=['Not Paraphrase', 'Paraphrase'])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
